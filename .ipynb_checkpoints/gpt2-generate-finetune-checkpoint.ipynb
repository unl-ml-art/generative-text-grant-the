{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7LoMj4GA4n_"
   },
   "source": [
    "#  GPT-2 Generation and Fine-Tuning\n",
    "\n",
    "This notebook explores GPT-2 (Generative Pretrained Transformer-2) from OpenAI. Read more about it [here](https://openai.com/blog/better-language-models/).\n",
    "\n",
    "Activities include:\n",
    "\n",
    "0. Setup\n",
    "1. Generate samples from pre-trained gpt-2model\n",
    "2. Fine-tune gpt-2 on text of your choosing. \n",
    "\n",
    "Adapted by Robert Twomey (rtwomey@unl.edu) for Machine Learning for the Arts SP22 from this [google colab](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) by [Max Woolf](http://minimaxir.com). See his repo [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run once to install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q gpt-2-simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart the kernel and run the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KBkpRgBCBS2_"
   },
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "import tensorflow as tf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj2IJLHP3KwE"
   },
   "source": [
    "## GPU\n",
    "\n",
    "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
    "\n",
    "You can verify which GPU is active by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUmTooTW3osf",
    "outputId": "c9fcfa4f-277d-4b3e-8974-373066dc157b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 21 15:32:51 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    27W / 250W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note the memory usage (0MiB / 32510MiB) for the Tesla V100.\n",
    "You can re-rerun the above cell to see what memory your code/models are using during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wXB05bPDYxS"
   },
   "source": [
    "## Downloading GPT-2\n",
    "\n",
    "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
    "\n",
    "There are three released sizes of GPT-2:\n",
    "\n",
    "* `124M` (default): the \"small\" model, 500MB on disk.\n",
    "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
    "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
    "* `1558M`: the \"extra large\", true model. Will not work if a K80/P4 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
    "\n",
    "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
    "\n",
    "The next cell downloads it from Google Cloud Storage and saves it in the the current working directory at `/models/<model_name>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8wSlgXoDPCR",
    "outputId": "10fc0d7c-d18f-4e11-a2af-bfade8b537eb"
   },
   "outputs": [],
   "source": [
    "model_name = \"355M\" # largest model we can fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run once to download the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 336Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 4.17Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 995Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 1.42Git [01:05, 21.7Mit/s]                                 \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 1.35Git/s]                                               \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 4.77Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 5.43Mit/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "gpt2.download_gpt2(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQAN3M6RT7Kj"
   },
   "source": [
    "# 1. Generate Text From The Pretrained Model\n",
    "\n",
    "If you want to generate text from the pretrained model pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`. (This is currently the only way to generate text from the 774M or 1558M models with this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "BAe4NpKNUj2C",
    "outputId": "b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 15:34:12.493053: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-21 15:34:13.117623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model models/355M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.load_gpt2(sess, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from the model\n",
    "The follow cell samples from gpt-2, using the provided prefix (seed) and other parameters. It starts the TF session and generates the samples.\n",
    "\n",
    "Try changing the parameters below to change the output: \n",
    "- `prefix` is the prompt. This will be the starting string/seed for your generation. Use your own text. \n",
    "- `temperature` sets the variability/randomness of the output. Range 0.0-1.0\n",
    "- `length` sets the lenght of output (in tokens). max is 1024.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "-xInIZKaU104",
    "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kansas City, Missouri)\n",
      "\n",
      "26. Buffalo, New York (Buffalo, New York)\n",
      "\n",
      "27. Columbus, Ohio (Columbus, Ohio)\n",
      "\n",
      "28. San Diego, California (San Diego, California)\n",
      "\n",
      "29. Richmond, Virginia (Richmond, Virginia)\n",
      "\n",
      "30. Wichita Falls, Texas (Wichita Falls, Texas)\n",
      "\n",
      "31. New Orleans, Louisiana (New Orleans, Louisiana)\n",
      "\n",
      "32. Los Angeles, California (Los Angeles, California)\n",
      "\n",
      "====================\n",
      "Kansas City, Missouri\n",
      "\n",
      "Name: M.J. Wilson\n",
      "\n",
      "Position: Defensive end\n",
      "\n",
      "Height: 6-2\n",
      "\n",
      "Weight: 238\n",
      "\n",
      "Age: 22\n",
      "\n",
      "College: Alabama\n",
      "\n",
      "Round(s): 7\n",
      "\n",
      "Drafted: 2012, No. 100 overall, by the New England Patriots\n",
      "\n",
      "The Patriots signed him to a four-year contract worth $12 million. But Wilson didn't play much in 2013, as he missed four games with a torn ACL. He was inactive\n",
      "====================\n",
      "Kansas City, Missouri, USA\n",
      "\n",
      "September 16, 2014\n",
      "\n",
      "\n",
      "A juvenile male was observed with a large white or dark patch on his upper left arm. His arm was extremely large and pointed. The patch was a little large and had a couple of small dark spots on the outside. The patch was visible from above. The patch was removed after 10 days.<|endoftext|>The following blog post, unless otherwise noted, was written by a member of Gamasutra's community.\n",
      "\n",
      "The thoughts and opinions expressed are those\n",
      "====================\n",
      "Kansas City, Missouri, July 19, 2012. (Photo: Eric Gay, AP)\n",
      "\n",
      "The latest news from the case is that the police officer who shot Michael Brown last August in Ferguson, Mo., is now on paid leave.\n",
      "\n",
      "The Associated Press reports that Brown's mother, Lesley McSpadden, has filed a lawsuit in federal court in St. Louis. McSpadden is seeking $5 million in damages for Brown's death, alleging that Officer Darren Wilson violated his civil rights and that he\n",
      "====================\n",
      "Kansas City, Missouri, USA\n",
      "\n",
      "March 24, 2006\n",
      "\n",
      "Size: 1 mm\n",
      "\n",
      "\n",
      "I was watching this in the video below. It's a very nice one.<|endoftext|>The following is a list of characters, places, and things from the video game Fallout: New Vegas.\n",
      "\n",
      "Contents show]\n",
      "\n",
      "Behind the scenes Edit\n",
      "\n",
      "Fallout 3 Edit\n",
      "\n",
      "The Fallout: New Vegas expansion adds a new location called the Vault of the Damned in the Mojave Wasteland. It is a vault\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess,\n",
    "              model_name=model_name,\n",
    "              prefix=\"Kansas City, Missouri\",\n",
    "              length=100,\n",
    "              temperature=0.7,\n",
    "              top_p=0.9,\n",
    "              nsamples=5,\n",
    "              batch_size=5\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activities\n",
    "- try varying the prefix. \n",
    "  - what length of prefix works best with the given model? \n",
    "  - how does the choice of prefix change the format/form of the output.\n",
    "- try varying the temperature.\n",
    "- try loading the different sized models (124M, 355M, 774M, 1558M) and generate text without changing the other parameters. \n",
    "  - Do you notice any qualitative differences in the output? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine-Tuning GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already generated with gpt2, you need to reset the tf graph and gpt2 session. Otherwise, we create a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeXshJM-Cuaf",
    "outputId": "a3c75caa-917b-4818-ca2d-d78610d8b6f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 15:35:31.227388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"355M\" # same model as selected above\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# check if sess exists (e.g. if we ran section 1 above)\n",
    "var_exists = 'sess' in locals() or 'sess' in globals()\n",
    "\n",
    "if not var_exists:\n",
    "    sess = gpt2.start_tf_sess()\n",
    "else:\n",
    "    sess = gpt2.reset_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeeSKtNWUedE"
   },
   "source": [
    "## Upload a text file\n",
    "For this, we will use a text file you provide to finetune (continue training) GPT-2. You can use any plain text (.txt) file. \n",
    "\n",
    "Simply drag and dropy our text file into the file browser at left. \n",
    "\n",
    "Once you have uploaded your file, update the file name in the cell below, then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6OFnPCLADfll"
   },
   "outputs": [],
   "source": [
    "file_name = \"UFO sightings.rtf\" # your file here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdpZQXknFNY3"
   },
   "source": [
    "## Run the finetuning\n",
    "\n",
    "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
    "\n",
    "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every `save_every` steps (can be changed) and when the cell is stopped.\n",
    "\n",
    "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them. If your input text is smaller, training might proceed more quickly.\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
    "\n",
    "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
    "* **`sample_every`**: Number of steps to print example output\n",
    "* **`print_every`**: Number of steps to print training progress.\n",
    "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
    "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
    "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For larger models, the recommended finetune() parameters are:\n",
      "\tuse_memory_saving_gradients = True\n",
      "\tonly_train_transformer_layers = True\n",
      "\taccumulate_gradients = 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 15:35:42.842697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/355M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 40695 tokens\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 | 29.32] loss=1.97 avg=1.97\n",
      "[20 | 49.26] loss=1.80 avg=1.88\n",
      "[30 | 69.18] loss=1.69 avg=1.82\n",
      "[40 | 89.10] loss=1.65 avg=1.78\n",
      "[50 | 109.03] loss=1.61 avg=1.74\n",
      "[60 | 128.94] loss=1.52 avg=1.70\n",
      "[70 | 148.86] loss=1.49 avg=1.67\n",
      "======== SAMPLE 1 ========\n",
      " a more general, or even generalizing, meaning. For \"pardon,\" the Oxford English Dictionary says \"to be granted and allow to be granted or allowed to be granted\" but then cites two other Oxford dictionaries, the British and American. There is one more Oxford dictionary, too: the Oxford New International Dictionary, and it has no suggestion of using the general usage of all dictionaries. Perhaps they will give it a closer look in time.\n",
      "\n",
      "The first mention of \"hearing\" in the New New York Times and elsewhere comes in a letter, Oct. 9, to the correspondent of the New York Daily News. It is from a lawyer in the firm of H. N. T. Gorman, and it is from Mr. Gorman to the reporter:\n",
      "\n",
      "\"In the old days before the State was so organized, the State Department, having two secretaries, one a lawyer, the other an accountant, could not have more than two persons in the Department, and some of them could not go to one of them while at the same time having the use of their own time. Now they have a three-man department, which has two officers, one a lawyer, the other an accountant. I have the honor to be a member. My chief of staff is a brilliant man, Mr. A. J. Deutsch, who is a lawyer or attorney-at-law. My assistant is an engineer, and my assistant director, Mr. A. L. Hough. Many of them have been through the State Department and can name the officers and the details.\"\n",
      "\n",
      "But this letter was issued in an era when it was not unusual to be asked to look at files as a State Department staff member. The letter was in the mail, and we can see that the letterhead was not the official State Department.\n",
      "\n",
      "We can see in the letter the old-fashioned attitude toward official information:\n",
      "\n",
      "\"We will use it as we see fit, if it's to complete the assignment.\"\n",
      "\n",
      "In the letter, Mr. T. T. Gorman is very clear and very honest. He was a lawyer who had been in an honest professional way; however, in this case he was using the term \"hearing\" to refer to an entirely different kind of information. It seemed to me that the State Department would take this matter very seriously. It seems to me that the State Department was much more concerned about the accuracy of it being made public—because, as it now stands before the public, it was quite embarrassing—that it could not be made public. The department is making much of the fact that no witnesses were interviewed in the case. I think it should be pointed out that the witness in that matter had gone before Mr. T. T. Gorman and others, very recently, and said that for a long time the names of his clients had not been known to his attorneys, who were at that time Mr. Gorman's assistants. He was, after all, a great man of honor, and he gave his own name and address, and that was all. But still after Mr. Gorman came in last April, for an official inspection, he was given information as to his client's client's identity. I'm sure the state department would be very interested in what the information is in this particular instance. . . . (p. 579)\n",
      "\n",
      "It all adds up to this: (a) the information would not be in the public interest, no matter how great the risks involved, when Mr. Gorman could \"use it as he sees fit,\" and (b) what was in the files at the time (Sept. 14) was not at all in the public benefit interest.\n",
      "\n",
      "The official record on the case is very clear: The information in the files referred to it but was not made publicly available.\n",
      "\n",
      "I now wish to refer to some further items that make the case clear that this practice is not consistent with our attitude with State Department documents. The following were provided by an official working for the State Department in its handling of the case:\n",
      "\n",
      "(a) the letter of Sept. 14 that Mr. Gorman sent to Mr. Hough and Mr. T. T. Gorman and sent an official to an inspector, and in the latter's report to then State Secretary W. B. Hunt for \"redactions.\"\n",
      "\n",
      "There are a lot more, but the point is clear enough. Mr. T. T. Gorman was the State Department's point of contact for the files in the case. Mr. Gorman used State Department contacts to try to make them public. His use was not without the usual problems, for the official to make a statement on Sept. 15 when he learned that two or three others, including two State Department employees had used the files for their own purposes, and the State Department official to say that the matter was closed. Mr. Hunt's statement was not the end of the matter. He returned the files\n",
      "\n",
      "[80 | 186.83] loss=1.30 avg=1.62\n",
      "[90 | 206.77] loss=1.34 avg=1.59\n",
      "[100 | 226.69] loss=1.32 avg=1.56\n",
      "[110 | 246.60] loss=1.29 avg=1.54\n",
      "[120 | 266.52] loss=1.26 avg=1.51\n",
      "[130 | 286.43] loss=1.27 avg=1.49\n",
      "[140 | 306.34] loss=1.16 avg=1.47\n",
      "======== SAMPLE 1 ========\n",
      "  \n",
      "       one round, oblong object with a circular top, red or orange lights on the water, and a flashing red light moving through the water.  Sighting lasted about 1 minute.  Explanation: radar.  Witness:  pilot of air tanker.  Witness:  One round, oblong object, 10' long, 4' thick with red lights on either side, and an exhaust with blue flashes as it passed overhead.  Sighting lasted 5 minutes.  A total of 18 aircraft were involved.  Note:  The sighting caused several pilots to stop their aircraft and take off for a safe altitude.  No further information found.\"       June, 1966; Chicago, Illinois.  8:45 p.m.  Witnesses:  graduate students of the Aeronautical and Astronautical Engineering College at nearby Brandeis University.  One brilliant, orange light, which varied in brightness but seemed to orbit the object for a few seconds, then fade, was seen for 10 minutes.  The witness' car was hit by a car, but was not damaged.       July, 1966; Middletown, New Jersey.  2:09 a.m.  Witnesses:  radar operators William and Mary Stilwell.  Radar tracked an object, like a homing deer which varied in brightness, from a distance of 100 to 400 miles.  Sound was heard at different times during the tracking sequence.       Aug, 1966; Washington, D.C. Witnesses:  USAF F/A- 18G lead F-86D escort pilot Lt. James McCauley and two other enlisted airmen.  Several bright white balls flew in an arc for 4 hours.       Sept., 1966; Wichita Falls, Texas. 7:07 p.m.  Witnesses:  pilot and gunner of an aircraft.  One object which was illuminated by a second object moved away from the first at high speed and then hovered for a few moments, stopping again to hover.  The aircraft was unable to take off.  Case missing from official files.       Oct., 1966; Kirtland AFB, New Mexico. 4:50 p.m.  Witness:  USAF employee James Moulton.  Four lights, which moved away from each other at high speed, moved to the south during the 5 minute sighting.  No further information found.       Nov. 26, 1966; Newburyport, Massachusetts. 9:30 p.m.  Witness:  one Mr J.D. Rau.  Three lights, which moved away from each other, then settled into place, then back again during the hour sighting.  No further information found.       Feb., 1967; Lakeland, Florida. 10:05 p.m.  Witness:  Mr. K.C. Gant, of New Orleans, La.  One dull, glowing orange ball, varying from reddish to bluish in color, made a humming sound and moved across the sky for a minute or so.  No further information found.       Aug. 27, 1967; San Antonio, Texas. 4:15 a.m.  Witnesses:  two U. S. Marshals.  One dark object, like a cylinder, hovered over the road for 30 seconds, then hovered over another nearby object.  No further information found.       Sept. 5, 1967; Chicago, Illinois. 10:30 p.m.  Witness:  radar technician Mr. and Mrs. Albert Hovde.  One orange, triangular object with a long nose, flew very fast and straight for 40 seconds.  Sighting lights and object slowed after 10 minutes.  No further information found.       Oct. 24, 1967; St. Louis, Missouri.  9:45 a.m.  Witness:  Mr. and Mrs. J.A. McBride.  One round, white disc with three flat edges at its center, made a high, metallic rumbling sound and made very fast circles during the 1 hour sighting.  No further information found.       Nov. 9, 1967; Killeen, Texas. 1:35 a.m.  Witness:  Mr. and Mrs. R.C. Williams.  One bright red, flashing light went straight out and then went back in.  It came within 30 seconds.  No further information found.       March, 1968; Dallas, Texas.  7:35 a.m.  Witness:  Mrs. A.C. Smith.  One round, flat disc with flashing lights, which varied in color from light brown to green, flew straight and level during 5 minute sighting.  No further information\n",
      "\n",
      "[150 | 340.94] loss=1.23 avg=1.45\n",
      "Saving checkpoint/run1/model-150\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name=model_name,\n",
    "              steps=150,\n",
    "              restore_from='fresh', # change to 'latest' to resume\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              learning_rate=1e-5,\n",
    "              sample_every=70,\n",
    "              save_every=150\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXSuTNERaw6K"
   },
   "source": [
    "## Notes on finetuning\n",
    "\n",
    "Keep an eye on the loss, and how quickly it is dropping. A too-rapid drop in loss could be a sign of overfitting, and a learning rate (lr) that is too high. \n",
    "\n",
    "After the model is trained, you can download the checkpoint folder to save your work. Training checkpoints are saved to `checkpoint/run1` (or whatever you chose for the run name above).\n",
    "\n",
    "You can compress it to a rar file and download that. Ask the instructor how.\n",
    "\n",
    "You're done! Feel free to go to the Generate Text From The Trained Model section to generate text based on your retrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune some more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already generated with gpt2, you need to reset the tf graph and gpt2 session. Otherwise, we create a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeXshJM-Cuaf",
    "outputId": "a3c75caa-917b-4818-ca2d-d78610d8b6f2"
   },
   "outputs": [],
   "source": [
    "#model_name = \"355M\" # same model as selected above\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# check if sess exists (e.g. if we ran section 1 above)\n",
    "var_exists = 'sess' in locals() or 'sess' in globals()\n",
    "\n",
    "if not var_exists:\n",
    "    sess = gpt2.start_tf_sess()\n",
    "else:\n",
    "    sess = gpt2.reset_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fine-tune some more, run the following. Be sure to increase the number of steps (if it was `500` before, change to `1000` to train for 500 more. the number is cumulative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name=model_name,\n",
    "              steps=300,\n",
    "              restore_from='latest', # change to 'latest' to resume\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              learning_rate=1e-5,\n",
    "              sample_every=100,\n",
    "              save_every=100\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClJwpF_ACONp"
   },
   "source": [
    "# 3. Generate Text From The Finetuned Model\n",
    "\n",
    "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RNY6RBI9LmL",
    "outputId": "82574eaa-d39a-4665-b611-e5172848da57"
   },
   "outputs": [],
   "source": [
    "# gpt2.generate(sess, run_name='run1') # no prefix, unconditional generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March, 23rd 2002. 2.45 pm. 10 miles from runway, 5,000 feet. A loud whizzing sound was heard for 10 seconds. No further details in files. Witness: R.T. Washburn, flying a Cessna 172.\n",
      "\n",
      "\n",
      "5 July 2002; Goose Bay, Labrador. 1.35 am. Witnesses: 1 pilot, 1 radar observer. Two objects, about the size of a pick-up truck, flew from north to south, with a tail thrust, for about 1 hour.\n",
      "\n",
      "\n",
      "29 April 2002; Crater Lake, Michigan. 11:35 p.m. Witness: N/A. One round, bright-green object flew erratically for 20 minutes, veered, and made an emergency landing at the bottom of the lake. ____________________\n",
      "\n",
      "\n",
      "28 May 2002; near Laredo, Texas. 8:30 p.m. Witness: ************. One object, as big as the Saturn V, flew straight and level for a few seconds, then gave an abrupt jerky, jerky climb. ____________________\n",
      "\n",
      "\n",
      "23 June 2002; Richmond, Virginia. 6:30 a.m. Witness: ************. Three flat, star-like objects, with tops like dominoes, flew very fast, jerky, erratically, through the air for 3-4 minutes. ____________________\n",
      "\n",
      "\n",
      "25 June 2002; St. Louis, Missouri. 7:30 p.m. Witnesses: ************. Three lights blinked, then exploded. ____________________\n",
      "\n",
      "\n",
      "29 June 2002; Ketchikan, Alaska. 10:35 p.m. Witnesses: ************.  Witness:  pilot of Cessna 172 that flew overhead during sighting. ____________________\n",
      "\n",
      "\n",
      "23 July 2002; Nacogdoches, Texas. 7:15 p.m.  Witness:  witness under hypnosis.  One dark object with a bright dome flew straight and level, for 15-20 seconds. ____________________\n",
      "\n",
      "\n",
      "26 July 2002; southeast of Little Rock, Arkansas. 9 p.m.  Witness:  USAF C-124 transport pilot/2d Lt. Bill Robinson.  One object with 3 lights that blinked changed color to gold, to orange to red to blue during 5 minute sighting. ____________________\n",
      "\n",
      "\n",
      "28 July 2002; Carlsbad, California. 9:20 p.m.  Witness:  witness in a car.  One yellow light flew straight and level for 20 minutes. ____________________\n",
      "\n",
      "\n",
      "30 July 2002; Tyndall AFB, Florida. 4:16 a.m.  Witness:  USAF B-52 jet fighter pilot/2nd Lt. Jim Donner.  One yellow light flew straight and level, for 4-5 minutes. ____________________\n",
      "\n",
      "\n",
      "1 August 2002; San Antonio, Texas. 7:32 p.m.  Witness:  flying an F-106 jet fighter. Witness' wife:  USAF Capt. M/Sgt. Alvarado.  One luminous object with two lights flew straight and level for 3 minutes. ____________________________________________________\n",
      "\n",
      "\n",
      "30 August 2002; near Castille, Texas. 7:25 a.m.  Witness:  pilot of C-124 transport plane.  One light moved erratically in front of the plane, then changed colors. ____________________\n",
      "\n",
      "\n",
      "29 August 2002; near Keflavik, Iceland. 5:50 a.m.  Witness:  pilot of C-124.  One round, high-altitude object with a dome, bright orange, and a trailing edge, flew straight and level, for 20 minutes. ____________________\n",
      "\n",
      "\n",
      "3 September 2002; Paine Field, Maine. 11:30 p.m.  Witness:  S/Sgt. Craig. One dark, three-dimensional object flew straight and level for a few seconds. ____________________\n",
      "\n",
      "\n",
      "15 September 2002; near Grand Island, Nebraska. 10:30 p.m.  Witness:  pilot of C-124.  One object with two lights flew straight and level, for 10 seconds. ____________________\n",
      "\n",
      "\n",
      "16 September 2002; near Grand Island, Nebraska. 10:40 p.m.  Witness:  pilot of C-124.  One object, like two lights, flew straight and level for a few seconds. ____________________\n",
      "\n",
      "\n",
      "25 September 2002; near Grand Island, Nebraska. 10:40 p.m.  Witness:  pilot of C-124.  One round, bright-green object, with a white, glowing, disc, flew straight and level, for 10 seconds. ____________________\n",
      "\n",
      "\n",
      "27 September 2002; near Johnston AFB, Nebraska. 7:05 p.m.  Witness:  pilot of USAF B-2 Spirit jet bomber.  One object with four lights flew straight and level for 1.5-2.5 minutes. ____________________\n",
      "\n",
      "\n",
      "3 October 2002;\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, run_name='run1', prefix=\"March, 23rd 2002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF4-PqF0Fl7R"
   },
   "source": [
    "## Notes\n",
    "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
    "\n",
    "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
    "\n",
    "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
    "\n",
    "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
    "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
    "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
    "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
    "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
    "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DKMc0fiej4N",
    "outputId": "490a4648-d973-4675-cf9a-7a48c16fd736"
   },
   "outputs": [],
   "source": [
    "gpt2.generate(sess,\n",
    "              length=250,\n",
    "              temperature=0.85,\n",
    "              prefix=\"Kansas City, Missouri\"\n",
    "              nsamples=5,\n",
    "              batch_size=5\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjjEN2Tafhl2"
   },
   "source": [
    "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
    "\n",
    "You can rerun the cells as many times as you want for even more generated texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fa6p6arifSL0"
   },
   "outputs": [],
   "source": [
    "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
    "\n",
    "gpt2.generate_to_file(sess,\n",
    "                      destination_path=gen_file,\n",
    "                      length=500,\n",
    "                      temperature=0.7,\n",
    "                      nsamples=100,\n",
    "                      batch_size=20\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-LRex8lfv1g"
   },
   "source": [
    "Download the file by hand in the browser at left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Trained Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTa6zf3e_9gV"
   },
   "source": [
    "Uploaded your saved checkpoint and unzip it.\n",
    "\n",
    "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
    "\n",
    "This will reset or start the tensorflow session as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fxL77nvAMAX",
    "outputId": "8938432a-3b86-4102-f32b-362721ecb897"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "if not sess:\n",
    "    sess = gpt2.start_tf_sess()\n",
    "else:\n",
    "    sess = gpt2.reset_session(sess)\n",
    "\n",
    "gpt2.load_gpt2(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ig-KVgkCDCKD"
   },
   "source": [
    "# Etcetera\n",
    "\n",
    "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIHiVP53FnsX"
   },
   "outputs": [],
   "source": [
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmTXWNUygS5E"
   },
   "source": [
    "# License\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 Max Woolf\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- Max's [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
    "- Original repo: [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple) by [Max Woolf](http://minimaxir.com). \n",
    "- Original [google colab](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) from Max."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Train a GPT-2 Text-Generating Model w/ GPU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TensorFlow GPU 2.6 (py39)",
   "language": "python",
   "name": "tensorflow-gpu-2.6-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
